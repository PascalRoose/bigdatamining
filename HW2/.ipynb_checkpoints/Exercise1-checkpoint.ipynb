{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session/application\n",
    "spark = SparkSession.builder.appName('Exercise 1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import News_Final.csv as dataframe using the defined schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"IDLink\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Headline\", StringType(), True),\n",
    "        StructField(\"Source\", StringType(), True),\n",
    "        StructField(\"Topic\", StringType(), True),\n",
    "        StructField(\"PublishDate\", TimestampType(), True),\n",
    "        StructField(\"SentimentTitle\", FloatType(), True),\n",
    "        StructField(\"SentimentHeadline\", FloatType(), True),\n",
    "        StructField(\"Facebook\", FloatType(), True),\n",
    "        StructField(\"GooglePlus\", FloatType(), True),\n",
    "        StructField(\"LinkedIn\", FloatType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "               .option(\"header\", \"true\")\\\n",
    "               .option(\"delimiter\", \",\")\\\n",
    "               .option('quote', '\"')\\\n",
    "               .option('escape', '\"')\\\n",
    "               .schema(schema)\\\n",
    "               .load(\"data/News_Final.csv\")\n",
    "\n",
    "# Make the values of title and headline lowercase \n",
    "#  and remove any non alphabetic characters (except spaces)\n",
    "# Remove hh:mm from publish date\n",
    "df = df.withColumn('Title', lower(col('Title')))\\\n",
    "       .withColumn('Title', regexp_replace('Title', '[^a-z\\s]', ''))\\\n",
    "       .withColumn('Headline', lower(col('Headline')))\\\n",
    "       .withColumn('Headline', regexp_replace('Headline', '[^a-z\\s]', ''))\\\n",
    "       .withColumn(\"PublishDate\", date_format(\"PublishDate\", \"yyyy-MM-dd\"))\n",
    "    \n",
    "### search values for the 2 cases\n",
    "cases = ('Headline', 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words and their frequency. Outputs a file for title and headline\n",
    "\n",
    "def getDf_Total(case):\n",
    "    ### Counts the words, sorts them from high to low\n",
    "    df_total = df.withColumn('word', explode_outer(split(col(case), ' ')))\\\n",
    "                 .groupBy('word')\\\n",
    "                 .count()\\\n",
    "                 .sort('count', ascending=False)\n",
    "    \n",
    "    return df_total\n",
    "\n",
    "def writeDf_Total(df_Total, case):\n",
    "    ### Writes the output into a file\n",
    "    ### Sparks partitions the output, we will rectify this later\n",
    "    df_Total.write.mode(\"overwrite\")\\\n",
    "                 .option(\"delimiter\", \" \")\\\n",
    "                 .csv(f'output/exercise1/total/{case}/partitions/Exercise1_{case}_Total')\n",
    "\n",
    "    ### Remove old instance of output file\n",
    "    os.system(f'rm output/exercise1/total/{case}/partitions/Exercise1_{case}_Total.txt')\n",
    "    ### Concat al the partitions into an txt file\n",
    "    os.system(f'cat output/exercise1/total/{case}/partitions/Exercise1_{case}_Total/p*'\\\n",
    "              f'> output/exercise1/total/Exercise1_{case}_Total.txt')\n",
    "\n",
    "\n",
    "for case in cases:\n",
    "    ### Get dataframe with the words counted and sorted\n",
    "    df_temp = getDf_Total(case)\n",
    "    ### Write the output to a file\n",
    "    writeDf_Total(df_temp, case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words per day (in title and headline) and their frequency. \n",
    "# Outputs a file for title and headline for every day\n",
    "\n",
    "def processDf_perDay(df_total, case):\n",
    "    ### Makes a list of dataframes\n",
    "    ### Each new dataframes contains the articles published on the same day\n",
    "    \n",
    "    ### Collects unique dates\n",
    "    unique_dates = df.agg(collect_set(\"PublishDate\")).collect()[0][0]\n",
    "    \n",
    "    ### List to store dataframes for articles published on the same day\n",
    "    df_dates_temp = {}\n",
    "\n",
    "    for date in unique_dates:\n",
    "        df_dates_temp[date] = df_total.filter(col('PublishDate') == date)\n",
    "    \n",
    "    df_dates = {}\n",
    "    \n",
    "    for date, df_date in df_dates_temp.items():\n",
    "        df_dates[date] = df_date.withColumn(\"word\", explode(split(col(case), \" \")))\\\n",
    "                                .groupBy(\"word\")\\\n",
    "                                .count()\\\n",
    "                                .sort(\"count\", ascending=False)\n",
    "    \n",
    "    return df_dates\n",
    "\n",
    "def writeDF_PerDay(df_date, date, case):\n",
    "    ### Write the output to a file\n",
    "    ### Sparks partitions the output, we will rectify this later\n",
    "    df_date.write.mode(\"overwrite\")\\\n",
    "                   .option(\"delimiter\", \" \")\\\n",
    "                   .csv(f'output/exercise1/perDay/partitions/Exercise1_{case}_{date}')\n",
    "    \n",
    "    ### Remove old instance of output file\n",
    "    os.system(f'rm output/exercise1/perDay/Exercise1_{case}_{date}.txt')\n",
    "    ### Concat al the partitions into an txt file\n",
    "    os.system(f'cat output/exercise1/perDay/partitions/Exercise1_{case}_{date}/p*'\\\n",
    "              f' > output/exercise1/perDay/Exercise1_{case}_{date}.txt')\n",
    "\n",
    "\n",
    "for case in cases:\n",
    "    ### Count the words in each dataframe\n",
    "    df_dates = processDf_perDay(df, case)\n",
    "    for date, df_date in df_dates.items():\n",
    "        ### Write the dataframe to a file\n",
    "        writeDF_PerDay(df_date, date, case)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words per topic and their frequency. Outputs a file for every topic\n",
    "\n",
    "def getDf_perTopic(topic, case):\n",
    "    ### Collects all the articles for the topic\n",
    "    df_temp_uncounted = df.filter(col('Topic') == topic)\n",
    "    \n",
    "    ### Counts, removes unwanted words ('...') & sorts from high to low\n",
    "    df_temp_counted = df_temp_uncounted.withColumn('word', explode(split(col(case), ' ')))\\\n",
    "                                       .groupBy('word')\\\n",
    "                                       .count()\\\n",
    "                                       .sort('count', ascending=False)\n",
    "    return df_temp_counted\n",
    "\n",
    "def write_perTopic(df_temp_counted, topic, case):\n",
    "    ### Write the output to a file\n",
    "    ### Sparks partitions the output, we will rectify this later\n",
    "    df_temp_counted.write.mode(\"overwrite\")\\\n",
    "                   .option(\"delimiter\", \" \")\\\n",
    "                   .csv('output/exercise1/perTopic/'+case+'/partitions/Exercise1_'+case+'_perTopic_'+topic)\n",
    "    \n",
    "    ### Remove old instance of output file\n",
    "    os.system('rm output/exercise1/perTopic/'+case+'/partitions/Exercise1_'+case+'_perTopic_'+topic+'.txt')\n",
    "    ### Concat al the partitions into an txt file\n",
    "    os.system('cat output/exercise1/perTopic/'+case+'/partitions/Exercise1_'+case+'_perTopic_'+topic+'/p* > output/exercise1/perTopic/'+case+'/Exercise1_'+case+'_perTopic_'+topic+'.txt')\n",
    "\n",
    "### Search values for the 4 topics\n",
    "### In the first place we wanted to look up the topics dynamically\n",
    "### But this gave way more topics then the 4 topics needed for the assignment\n",
    "### We therefore only search for articles with 1 of the 4 topics asked in the assignment\n",
    "topics = ('obama', 'economy', 'palestine', 'microsoft')\n",
    "\n",
    "### For both of the 2 cases\n",
    "for case in cases:\n",
    "    ### For all of the 4 topics\n",
    "    for topic in topics:\n",
    "        ### collect the topics, count and write\n",
    "        df_temp_counted = getDf_perTopic(topic, case)\n",
    "        write_perTopic(df_temp_counted, topic, case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
