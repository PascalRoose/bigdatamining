{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session/application\n",
    "spark = SparkSession.builder.appName('Exercise 3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import household_power_consumption.txt as dataframe using the defined schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"Time\", StringType(), True),\n",
    "        StructField(\"Global_active_power\", FloatType(), True),\n",
    "        StructField(\"Global_reactive_power\", FloatType(), True),\n",
    "        StructField(\"Voltage\", FloatType(), True),\n",
    "        StructField(\"Global_intensity\", FloatType(), True),\n",
    "        StructField(\"Sub_metering_1\", FloatType(), True),\n",
    "        StructField(\"Sub_metering_2\", FloatType(), True),\n",
    "        StructField(\"Sub_metering_3\", FloatType(), True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").schema(schema).load(\"household_power_consumption.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the normalized values and add them to the dataframe as a new column\n",
    "normalize = lambda xi, xmin, xmax : (xi - xmin) / (xmax - xmin)\n",
    "\n",
    "columns = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']\n",
    "for column in columns:\n",
    "    column_result = df.agg(\n",
    "        min(column).alias('min'), \n",
    "        max(column).alias('max') \n",
    "    ).collect()[0]\n",
    "    df = df.withColumn(column + '_norm', normalize(df[column], column_result['min'], column_result['max']))\n",
    "\n",
    "df_norm = df.select([column + '_norm' for column in columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the modified dataframe to csv. \n",
    "## Spark write function will split the workload and save the output spread out over multiple parts\n",
    "## Using cat and >  we will generate a single output file\n",
    "df_norm.write.mode(\"overwrite\").csv('household_power_consumption_normalized')\n",
    "os.system('rm household_power_consumption_normalized.txt')\n",
    "os.system('cat household_power_consumption_normalized/p* > household_power_consumption_normalized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
