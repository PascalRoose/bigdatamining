{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session/application\n",
    "spark = SparkSession.builder.appName('Term_Statistics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_hire_stats.csv as dataframe using the defined schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Zone_ID\", ByteType(), False),\n",
    "        StructField(\"Date\", StringType(), False),\n",
    "        StructField(\"Hour_slot\", ByteType(), False),\n",
    "        StructField(\"Hire_count\", ShortType(), False)\n",
    "    ]\n",
    ")\n",
    "train_df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(\"data/train_hire_stats.csv\")\n",
    "\n",
    "train_df = train_df.withColumn('Day_of_the_week', \n",
    "                               (F.date_format(train_df[\"Date\"], \"u\").cast(IntegerType())))\n",
    "\n",
    "train_df = train_df.withColumn('Month', \n",
    "                               (F.date_format(train_df[\"Date\"], \"M\").cast(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test_hire_stats.csv as dataframe using the defined schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Test_ID\", ShortType(), False),\n",
    "        StructField(\"Zone_ID\", ByteType(), False),\n",
    "        StructField(\"Date\", StringType(), False),\n",
    "        StructField(\"Hour_slot\", ByteType(), False),\n",
    "        StructField(\"Hire_count\", ByteType(), False)\n",
    "    ]\n",
    ")\n",
    "test_df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(\"data/test_hire_stats.csv\")\n",
    "\n",
    "test_df = test_df.withColumn('Day_of_the_week', \n",
    "                             (F.date_format(test_df[\"Date\"], \"u\").cast(IntegerType())))\n",
    "\n",
    "test_df = test_df.withColumn('Month', \n",
    "                             (F.date_format(test_df[\"Date\"], \"M\").cast(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = train_df.groupBy(\"Zone_ID\", \"Day_of_the_week\", \"Hour_slot\").mean(\"Hire_count\")\n",
    "compare_df = compare_df.withColumn(\"avg(Hire_count)\", compare_df[\"avg(Hire_count)\"].cast(IntegerType()))\n",
    "\n",
    "chinese_ny = train_df.where(\"Date == '2016-02-01'\").groupBy(\"Zone_ID\", \"Hour_slot\").mean(\"Hire_count\").orderBy(\"Zone_ID\", \"Hour_slot\")\n",
    "chinese_ny = chinese_ny.withColumn(\"avg(Hire_count)\", chinese_ny[\"avg(Hire_count)\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+---------+----------+\n",
      "|Test_ID|Zone_ID|      Date|Hour_slot|Hire_count|\n",
      "+-------+-------+----------+---------+----------+\n",
      "|      0|      7|2017-02-01|        0|         0|\n",
      "|      1|      7|2017-02-01|        1|         0|\n",
      "|      2|      7|2017-02-01|        2|         0|\n",
      "|      3|      7|2017-02-01|        3|         0|\n",
      "|      4|      7|2017-02-01|        4|         0|\n",
      "|      5|      7|2017-02-01|        5|         0|\n",
      "|      6|      7|2017-02-01|        6|         0|\n",
      "|      7|      7|2017-02-01|        7|         0|\n",
      "|      8|      7|2017-02-01|        8|         0|\n",
      "|      9|      7|2017-02-01|        9|         0|\n",
      "|     10|      7|2017-02-01|       10|         0|\n",
      "|     11|      7|2017-02-01|       11|         0|\n",
      "|     12|      7|2017-02-01|       12|         0|\n",
      "|     13|      7|2017-02-01|       13|         0|\n",
      "|     14|      7|2017-02-01|       14|         0|\n",
      "|     15|      7|2017-02-01|       15|         0|\n",
      "|     16|      7|2017-02-01|       16|         0|\n",
      "|     17|      7|2017-02-01|       17|         0|\n",
      "|     18|      7|2017-02-01|       18|         0|\n",
      "|     19|      7|2017-02-01|       19|         0|\n",
      "+-------+-------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = test_df.join(compare_df, [\"Zone_ID\", \"Day_of_the_week\", \"Hour_slot\"], \"fullouter\")\n",
    "final_df = final_df.withColumn(\"Hire_count\", final_df[\"avg(Hire_count)\"])\n",
    "final_df = final_df.select(\"Test_ID\", \"Zone_ID\", \"Date\", \"Hour_slot\", \"Hire_count\").filter(\"Test_ID is not null\").orderBy(F.asc(\"Test_ID\"))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the modified dataframe to csv. \n",
    "## Spark write function will split the workload and save the output spread out over multiple parts\n",
    "## Using cat and >  we will generate a single output file\n",
    "final_df.write.mode(\"overwrite\").csv('output/attempts/first-attempt')\n",
    "os.system('rm output/attempts/first-attempt.csv')\n",
    "os.system('cat output/attempts/first-attempt/p* > output/attempts/first-attempt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------+\n",
      "|Zone_ID|Hour_slot|avg(Hire_count)|\n",
      "+-------+---------+---------------+\n",
      "|     14|        0|              0|\n",
      "|     14|        1|              0|\n",
      "|     14|        2|              0|\n",
      "|     14|        3|              0|\n",
      "|     14|        4|              0|\n",
      "|     14|        5|              0|\n",
      "|     14|        6|              0|\n",
      "|     14|        7|              0|\n",
      "|     14|        8|             12|\n",
      "|     14|        9|              8|\n",
      "|     14|       10|              3|\n",
      "+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chinese_ny.select(\"Zone_ID\", \"Hour_slot\", \"avg(Hire_count)\").where(\"Zone_ID == 14 AND Hour_slot <= 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
